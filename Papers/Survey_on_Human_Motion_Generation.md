| 分类                | 工作名称                                     | 全称                                                                                                                   | 简述                                                                                  | 解决的问题                                                                                     | 方法                                                                                                                                                       | 备注                                                                 | online/offline |     |   |   |   |
|-------------------|--------------------------------------------|----------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------|----------------|-----|---|---|---|
| Motion Generation | CLOSD (2025)                               | CLOSD: CLOSING THE LOOP BETWEEN<br>SIMULATION AND DIFFUSION FOR<br>MULTI-TASK CHARACTER CONTROL                        | 把扩散模型与仿真环境中的RL结合起来，实现多任务场景下的交互式角色控制。                            | 1.动作扩散模型的离线到在线 2.基于RL的模型的泛化能力不足 3.计划与执行之间缺少闭环                                         | 1.DiP：用MDM进行在线的动作补全<br>2.πPHC物理控制                                                                                                                           | 窗口式的MDM动作片段生成实现扩散模型的在线生成，支持conditional generation。这里的在线MDM可以借鉴 | ONLINE         |     |   |   |   |
|                   | UniHSI (2024)                              | UniHSI: UNIFIED HUMAN-SCENE INTERACTION VIA PROMPTED CHAIN-OF-CONTACTS                                               |                                                                                      |                                                                                               |                                                                                                                                                            |                                                                |                |     |   |   |   |
|                   | InterPhys (2023)                            | Synthesizing Physical Character-Scene Interactions                                                                  |                                                                                      |                                                                                               |                                                                                                                                                            |                                                                |                |     |   |   |   |
|                   | MotionStreamer (2025)                       | MotionStreamer: Streaming Motion Generation via Diffusion-based<br>Autoregressive Model in Causal Latent Space       | 面向实时动作生成的潜在扩散自回归模型，可以实现序列text condition                               | 1.离线批量生成to实时交互场景 2.自回归模型适合流式生成，但容易积累误差 3.扩散模型生成质量高，但推理通常慢                             | 1.Causal TAE将动作编码到latent space<br>2.扩散自回归模型，在保证质量的同时实现流式推理                                                                                              | 希望使用其框架，直接使用其预训练的TAE模型来把动作编码到潜空间                               | ONLINE         |     |   |   |   |
|                   | T2M-GPT (2023)                              | T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations                               |                                                                                      |                                                                                               |                                                                                                                                                            |                                                                |                |     |   |   |   |
|                   | FlowMDM (2024)                              | FlowMDM: Seamless Human Motion Composition with<br>Blended Positional Encodings                                       |                                                                                      |                                                                                               |                                                                                                                                                            |                                                                |                |     |   |   |   |
|                   | DoubleTake (2024)                           | Human Motion Diffusion as a Generative Prior                                                                         |                                                                                      |                                                                                               |                                                                                                                                                            |                                                                |                |     |   |   |   |
|                   | A-MDM (2024)                                | Interactive Character Control with Auto-Regressive Motion Diffusion<br>Models                                        | 提出一种自回归扩散模型用于动作生成：通过当前帧输入扩散模型得到下一帧，实现连续流式动作生成，并结合强化学习实现可控的交互式角色控制。 | 1.传统扩散模型离线批量生成，难以满足实时交互需求<br>2.如何在保持高保真度和长时序多样性的同时，实现实时生成和任务导向控制                 | 1.基于条件扩散模型，输入初始姿态，逐帧自回归生成动作<br>2.使用简单的MLP网络架构，实现高效、轻量的动作序列生成<br>3.交互控制技术:任务导向采样、动作帧修补、分层强化学习 | 没有conditional generation。后续可以借鉴这里的控制策略                         | ONLINE         |     |   |   |   |
|                   | HuMoR (2021)                                | HuMoR: 3D Human Motion Model for Robust Pose Estimation                                                             |                                                                                      |                                                                                               |                                                                                                                                                            |                                                                |                |     |   |   |   |
|                   | MVAE (2020)                                 | Character Controllers Using Motion VAEs                                                                              |                                                                                      |                                                                                               |                                                                                                                                                            |                                                                |                |     |   |   |   |
|                   | Diffusion Policy (2024)                     | Diffusion Policy: Visuomotor Policy<br>Learning via Action Diffusion                                               | 将机器人的视觉运动策略表示为条件去噪扩散过程，实现通过扩散模型直接生成机器人动作。                          | 1.传统机器人策略在高维动作空间或多模态动作分布下表现不稳定<br>2.难以兼顾训练稳定性和多任务泛化能力<br>3.需要有效的策略生成方法以在物理机器人上实现可靠 visuomotor 控制 | 1.将机器人策略建模为条件去噪扩散过程<br>2.引入receding horizon control以提升实际控制的可行性<br>3.采用时间序列扩散Transformer处理连续动作序列，实现长期动作规划                        | 机械臂的自由度是比较低的                                                   | ONLINE         |     |   |   |   |
|                   | MDM (2023)                                  | HUMAN MOTION DIFFUSION MODEL                                                                                         | 输入条件（如文本或动作标签）后，MDM利用扩散模型自回归生成自然且富有表现力的人体动作序列，同时支持多种生成任务和条件方式。        | 1.生成自然且富有表现力的人体动作序列困难<br>2.需要将几何约束融入生成模型                                                     | 1.基于Transformer的扩散生成模型<br>2.在每个扩散步骤预测样本而非噪声，便于使用位置和速度上的几何损失                                                                                       |                                                                | OFFLINE        |     |   |   |   |
|                   | T2M (2022)                                  | Generating Diverse and Natural 3D Human Motions from Text                                                            |                                                                                      |                                                                                               |                                                                                                                                                            |                                                                |                |     |   |   |   |
|                   | TM2T (2022)                                 | TM2T: Stochastic and Tokenized Modeling for the Reciprocal Generation of 3D Human Motions and Texts                  |                                                                                      |                                                                                               |                                                                                                                                                            |                                                                |                |     |   |   |   |
|                   | T2M-GPT (2023)                              | T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations                               | VQVAE编码，GPT生成，text条件                                                          | 1.以前的方法对长或复杂的text生成效果差                                                             | 1.用VQVAE来tokenize（EMA，CodeReset）<br>2.用GPT生成                                                                                                             | 长点方法，用了mask transformer                                        | OFFLINE        |     |   |   |   |
|                   | MoMask (2024)                               | MoMask: Generative Masked Modeling of 3D Human Motions                                                               | 输入text，离线生成动作序列，用残差VQVAE来tokenize，利用两种transformer的结构分别建模base token和残差token   | 1.以前的方法保真度不够                                                                                | 1.用残差VQVAE来tokenize<br>2.用masked transformer建模base tokens<br>3.用residual transformer建模残差tokens<br>4.用decoder解码回动作                                              | 长点为主，diversity不够，离线，residual vqvae对建模细节有好处                     | OFFLINE        |     |   |   |   |
|                   |                                             |                                                                                                                      |                                                                                      |                                                                                               |                                                                                                                                                            |                                                                |                |     |   |   |   |
| HHI               | Ready to React (2025)                       | READY-TO-REACT: ONLINE REACTION POLICY FOR<br>TWO-CHARACTER INTERACTION GENERATION                                   | 在线反应策略，通过自回归+扩散模型实现两角色实时交互动作生成，使得角色能够像人类一样动态回应对方动作，针对单一动作       | 1.基于完整序列预测，缺乏对实时交互过程的建模<br>2.生成过程中容易累积误差<br>3.缺乏灵活的外部控制能力                                     | 1.为每个角色设计独立的反应策略<br>2.在自回归结构中引入扩散生成头，提升生成稳定性并减轻误差累积<br>3.通过稀疏控制信号调节动作                                                                 | 能实现单一视角的基于潜在扩散自回归的动作生成，但是没法用text控制生成，只支持单一动作                   | ONLINE         |     |   |   |   |
|                   | Finedual (2025)                             | fine-grained text-driven dual-human motion generation via hierachical interaction                                    |                                                                                      |                                                                                               |                                                                                                                                                            |                                                                | OFFLINE        |     |   |   |   |
|                   | SymBridge (2025)                            | SymBridge: A Human-in-the-Loop Cyber-Physical Interactive System for<br>Adaptive Human-Robot Symbiosis               | 人与机器人交互系统，结合增强现实与实时交互模型，实现自然、高效的人机共生                                             | 1.现有机器人模拟器无法真实引入人类参与<br>2.机器人缺乏对复杂人类行为与环境语境的实时适应能力                                           | 1.人机在环系统设计<br>2.基于多分辨率人体运动特征与环境可供性，实时生成机器人响应动作<br>3.持续学习与反馈机制                                                               | 输入人的动作输出机器人动作，在线。但是无法支持长序列，动作也不是特别自然                           | ONLINE         |     |   |   |   |
|                   | InterMask (2025)                            | InterMask: 3D Human Interaction Generation via Collaborative Masked Modelling                                        | 提出了一种基于离散空间协作掩码建模的3D人际交互生成方法，从文本描述生成高保真、高度协调的双人动作序列。                   | 1.多人体交互难度大<br>2.空间与时间依赖难以建模                                                      | 1.用VQVAE将每个动作序列编码为二维离散token map，既保留时间序列信息也捕捉身体部位的空间信息<br>2.使用生成式掩码建模框架同时建模两人的token                                            | 相比于InterGen有更加精细的控制                                            | OFFLINE        |     |   |   |   |
|                   | ReGenNet (2024)                             | ReGenNet: Towards Human Action-Reaction Synthesis                                                                  | 提出了基于diffusion的人类动作-反应生成方法，支持在线无label生成                                     | 强调原子交互动作角色身份的不对称性和实时性                                                               | transformer去噪，mask保证causal，DDIM加速，提出基于距离的显示交互损失                                                                                                  | 不支持长序列，缺少高质量标注动作-反应的数据集                                        | ONLINE         | wyx |   |   |   |
|                   | DuetGen (2025)                              | DuetGen: Music Driven Two-Person Dance Generation via Hierarchical Masked Modeling                                     | 提出音乐驱动的双人舞蹈生成框架，采用两层分层VQ-VAE将连续运动转换为离散token，用两阶段transformer来generate token  | 双人舞蹈动作交互要求的同步性和精密性高                                                                      | 双人运动作为整体建模（同时采用相对位移）表示，两层token捕获宏观语义和微观细节                                                                                             | vq-vae重建时用到了音乐特征作为条件信息                                         | OFFLINE        |     |   |   |   |
|                   | Two In One (2024)                           | Two-in-One: Unified Multi-Person Interactive Motion Generation by Latent Diffusion Transformer                        | 把双人动作嵌入latent，之后用DiT建模双人动作                                                       | 之前的工作是将建模单人动作再integrate成双人交互                                                          | 1.用InterVAE把双人动作嵌入latent<br>2.用DiT建模latent上的双人交互动作                                                                                                        | 是Pro版的InterGen，把DiT搬到了latent上，而且是直接嵌入的双人动作                     | OFFLINE        |     |   |   |   |
|                   | InterGen (2024)                             | InterGen: Diffusion-based Multi-human Motion Generation<br>under Complex Interactions                                 | 提出 InterGen，一个基于扩散模型的多人体动作生成方法，能够在复杂交互场景下，仅通过文本指导生成高质量的两人交互动作。        | 1.忽视多人体交互<br>2.身份对称性难题<br>3.全局关系建模不足<br>4.缺乏大规模数据                                           | 1.构建 InterHuman 数据集<br>2.共享参数的Transformer-based denoiser<br>3.对称性假设                                                                                          | 210帧DDIM50推理需要600ms每一帧，时间无法接受                                  | OFFLINE        |     |   |   |   |
|                   | TiMotion (2025)                             | TIMotion: Temporal and Interactive Framework for<br>Efficient Human-Human Motion Generation                           | TIMotion提出了一种高效的人-人交互动作生成框架，通过因果交互建模和角色动态适应机制，实现更流畅合理的双人动作生成          | 1.分别建模两人的动作缺乏对交互序列的建模<br>2.生成效果不足以及模型参数冗余                                            | 1.Causal Interactive Injection: 将两个独立的序列转化为因果序列<br>2.Role-Evolving Scanning: 在交互过程中动态调整主动与被动角色，适应角色切换<br>3.Localized Pattern Amplification: 捕捉短期局部动作模式，提升动作平滑性与合理性 | 其实就是进化版的InterGen，因果序列做的是改变输入序列的拼接方式                            | OFFLINE        |     |   |   |   |
|                   | InterFormer (2023)                          | Interaction Transformer for Human Reaction Generation                                                                |                                                                                      |                                                                                               |                                                                                                                                                            |                                                                |                |     |   |   |   |
|                   | Interactive Humanoid (2024)                 | Interactive Humanoid: Online Full-Body Motion Reaction Synthesis with Social Affordance Canonicalization and Forecasting |                                                                                      |                                                                                               |                                                                                                                                                            |                                                                |                |     |   |   |   |
|                   | InsActor (2023)                             | InsActor: Instruction-driven Physics-based Characters                                                                | InsActor是一个用扩散模型作为高层控制生成状态序列和vae作为低层控制生成action的生成框架，能够根据人类自然语言指令生成物理仿真的角色动画。 | 1.高层指令到低层动作的映射困难：难以保证物理可行性<br>2.直接生成动作缺乏稳定性<br>3.指令驱动的长期任务难以实现                          | 1.分层生成框架：在高层使用扩散策略生成指令条件下的动作规划，在低层通过技能学习保证动作的物理可行性<br>2.扩散模型驱动的高层规划<br>3.VAE低层技能发现与执行<br>4.通过物理引擎保证动作的合理性，同时在训练中对齐人类语言与动作表现 | 需要先用diffusion生成运动序列再用cVAE生成物理动作，无法在线，但是cVAE实现物理驱动相较于PHC是另一种思路。 | OFFLINE        |     |   |   |   |
|                   | PhysReaction (2024)                         | PhysReaction: Physically Plausible Real-Time Humanoid Reaction Synthesis via Forward Dynamics Guided 4D Imitation      | PhysReaction提出了一种基于前向动力学引导的4D模仿方法，实现了实时、物理合理的人形反应生成，使机器人能够自然、快速地对人类动作做出反应 | 1.物理不合理性<br>2.实时性不足                                                                        | 1.通过高精度运动捕捉和motion tracking，将动作数据转化为状态-动作对<br>2.前向动力学模型训练:使用状态与动作的VAE编码，实现对下一状态的预测<br>3.4D模仿学习:结合历史动作、当前状态和预测的下一状态，训练反应策略<br>4.迭代通用-专家策略 | 思路也是生成reaction并且有物理，但是没有text条件控制                               | ONLINE         |     |   |   |   |
|                   | InterControl (2024)                         | InterControl: Zero-shot Human Interaction Generation by Controlling Every Joint                                      | 基于扩散模型的零样本多人交互动作生成方法，利用LLM自动生成关节对约束条件，通过控制关节之间的距离来实现交互                        | 1.以往的动作空间控制不够精准<br>2.以往的HHI需要多人动作数据集                                                       | 1.LLM得到的全局空间中的关节位置控制信号来控制交互<br>2.ControlNet结构的为MDM注入对方交互信息                                                                                             | controlnet的结构可以借鉴                                              | OFFLINE        | wyx |   |   |   |
|                   | PriorMDM (2024)                             | PriorMDM: Human Motion Diffusion<br>as a Generative Prior                                                           |                                                                                      |                                                                                               |                                                                                                                                                            |                                                                |                |     |   |   |   |
|                   |                                             |                                                                                                                      |                                                                                      |                                                                                               |                                                                                                                                                            |                                                                |                |     |   |   |   |
|                   |                                             |                                                                                                                      |                                                                                      |                                                                                               |                                                                                                                                                            |                                                                |                |     |   |   |   |
| HOI               |                                             |                                                                                                                      |                                                                                      |                                                                                               |                                                                                                                                                            |                                                                |                |     |   |   |   |
|                   | SceneMI (2025)                              | SceneMI: Motion In-betweening for Modeling Human-Scene Interaction                                                  |                                                                                      |                                                                                               |                                                                                                                                                            |                                                                |                |     |   |   |   |
| HSI               |                                             |                                                                                                                      |                                                                                      |                                                                                               |                                                                                                                                                            |                                                                |                |     |   |   |   |
|                   | Awesome-Human-Interaction-Motion-Generation | A Survey on Human Interaction Motion Generation                                                                    |                                                                                      |                                                                                               |                                                                                                                                                            |                                                                |                |     |   |   |   |
| 其他相关工作            |                                             |                                                                                                                      |                                                                                      |                                                                                               |                                                                                                                                                            |                                                                |                |     |   |   |   |
|                   |                                             |                                                                                                                      |                                                                                      |                                                                                               |                                                                                                                                                            |                                                                |                |     |   |   |   |
